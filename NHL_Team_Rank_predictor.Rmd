---
title: "An Analysis of Team Success in relation to Player Qualities in the National Hockey League"
subtitle: "edX HarvardX: PH125.9x - Data Science: Capstone"
author: "Craig Hamlin"
date: "August 5, 2021"
output:
  pdf_document:
    toc: true
    toc_depth: 3
    number_sections: true

    
---
```{r setup, set.seed(43), include=FALSE}
#establish global setting for all r chunks in project
knitr::opts_chunk$set(cache = T)
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(message = FALSE)
```


```{r, Load Libraries, results='hide'}
#load applicable libraries for project
nhlpackages <- c("rpart.plot", "doBy", "janitor", "lubridate", "dplyr", "ggplot2", "tidyverse", "caret", "rvest", "xgboost", "e1071", "knitr", "rrcov")  

lapply(nhlpackages, function(i){ 
  if(!require(i, character.only = TRUE)){
    install.packages(i, repos = "http://cran.us.r-project.org") 
    library(i, character.only = TRUE)
}})

if(!require(tinytex)){
  install.packages("tinytex")
  library(tinytex)
  install_tinytex()
}
```


\newpage

# INTRODUCTION

In this project a publicly available dataset was employed to create machine learning models which predicted the rank of a National Hockey League player's team.

### National Hockey League Overview

The sport of ice hockey in the National Hockey League (NHL) consists of 32 teams of professional athletes that compete against each other within a points based match system. The results of each match produces a winner and a loser and the objective of every team is to win as many games as possible. A season of hockey consists of roughly a one year time period whereby the points system resets and the teams once again start a new season. The teams that obtain the highest winning percentages in each season can be considered to be in the elite, top stratum of the NHL. 
Imposed upon every team equally is a salary cap which is the total amount of money that a team can pay its players for a season. The salary cap has a dual purpose: to create parity in the league so rich teams cannot financially dominate over smaller market teams and to prevent player salaries from being inflated beyond league sustaining levels. 
Players that make up the team can play one of 5 different positions: Center, Left Wing, Right Wing, Defence and Goalie. For the sake of this project the position of Goalie was not included in the dataset as the performance statistics are not comparable with the other positions. 
Using 10 separate features unique to every player observation in our dataset, team rank was predicted through the machine learning models of Recursive Partitioning, Robust Linear Discriminate Analysis and eXtreme Gradient Boosting. The analysis of the data and results of the machine learning models are contained within this report.



# Methods and Analysis

## Data Wrangling and Cleaning


After importing the nhl player and team data with permission from hockeystatssupplier on github, the data was then cleaned and engineered to best suit the project. Of note: only players with a minimum of 20 games played per season were included in the player datasets. This minimum games threshold was important to implement in order to provide a larger sample size for player performance on a seasonal basis.

### Download Player Data and coerce into useable dataframe

Player data consists of two separate dataframes. Each contain similar columns such as player names, position, etc, however one dataframe contains salary cap based information such as cost per point, and the other dataframe contains other in-depth game based information like ice time per game and power play points totals.
The two separate dataframes were modified with regex to create a more workable resource by removing dollar signs, trimming white space and removing accents from letters. Also, the features cap hit and cost per point were weighted to account for the salary cap inflation that occurs yearly. The two data frames were then joined to be utilized later alongside the team data.

```{r, import and clean player data}

# Import player stats data
c <- c("2011-2012.csv", "2012-2013.csv", "2013-2014.csv", "2014-2015.csv","2015-2016.csv","2016-2017.csv","2017-2018.csv","2018-2019.csv","2019-2020.csv","2020-2021.csv")
url <- paste("https://github.com/HockeyStatSupplier/StatsSupply/blob/main/NHL_Player_Stats%20-%20",c,sep="")
player_stats <- lapply(url, function(i){
  l <- read_html(i)
  html_table(l)
})

#iterate through 'player_stats' to create column names from row one of df
players <- lapply(player_stats, function(i){   
  df <- as.data.frame(i) #convert list item to dataframe
  df %>% row_to_names(row_number = 1)  
})

#clean the 'players' data
p <- 1:10
players <- lapply(p, function(i){ 
  x <- as.data.frame(players[i]) #convert list item to dataframe
  # remove periods and commas with regex, trim white space from 'Player' column, convert 'TOI.GP' to seconds, select the columns that will be used in this project
  x %>%  mutate(TEAM = gsub(".*,(.*)", "\\1", x$Team), Player = str_trim(Player), Player = trimws(Player), TOI.GP = as.numeric(as.period(ms(x$TOI.GP), unit = "sec"))) %>% 
  select(TEAM,Player,Pos, Season,G,A,P,PPP, TOI.GP)
  })

```

```{r, import and clean player salary cap data}
# Import player salary cap data
url2 <- paste("https://github.com/HockeyStatSupplier/StatsSupply/blob/main/Player_Cap%20-%20",c,sep="")
cap_stats <- lapply(url2, function(i){
  l <- read_html(i)
  html_table(l)
})

#iterate through 'cap_stats' to create column names from row one of df
x <- lapply(cap_stats, function(i){
  df <- as.data.frame(i) #convert list item to dataframe
  df %>% row_to_names(row_number = 1) 
})

# vector of the salary cap (in millions of $) for each year 2011-2020
cap_by_year <- c(64.3,64.3,64.3,69,71.4,73,75,79.5,81.5, 81.5) 


# clean the salary cap data
capyears <- lapply(p, function(i){
  df <- as.data.frame(x[i]) #convert list item to dataframe
  df$CAP.HIT <- as.numeric(gsub("[\\$,]", "", df$CAP.HIT)) #removes dollar sign and comma from column
  df$X..P <- as.numeric(gsub("[\\$,]", "", df$X..P)) #removes dollar sign and comma from column
  df$CAP.HIT <- round(df$CAP.HIT*81.5/cap_by_year[i]) #adjust cap hit for inflation
   df$X..P <- round(df$X..P*81.5/cap_by_year[i]) #adjust cost per point for inflation
  # regex to fix the backward player listing and remove commas
  df <- df %>% mutate(firstname = gsub(".*,(.*)", "\\1", df$PLAYER), lastname =  gsub("(.*),.*", "\\1", df$PLAYER)) %>% mutate(Player = str_c(firstname, ' ', lastname)) %>%  select(Player, AGE,CAP.HIT, X..P, GP) %>% filter(GP >= 20)
  # trim the 'Player' column
  df <- df %>% mutate(Player = str_trim(Player), Player = trimws(Player))
  # replace euro letters in cap data that aren't found in the 'players' dataframe
  NAplayer <- df$Player %>% str_replace("ö", "o") %>% str_replace("ä", "a") %>%
    str_replace("ü", "u") %>% str_replace("á", "a") %>% str_replace("É", "E")      %>% str_replace("é", "e") 
  df %>% mutate(Player = NAplayer)
})

```

```{r, combine player and cap dataframes}

#JOIN THE TWO DATAFRAMES AND OMIT NA
master <- lapply(p, function(i){ 
  x <- as.data.frame(capyears[i])
  y <- as.data.frame(players[i])
  z <- full_join(x,y, by = "Player")
  na.omit(z)
})
```


```{r, display top five rows of cleaned player data}
kable(head(as.data.frame(master[1])))
```


### Download Team Data and coerce into useable dataframe

Team data consists of team performance data from years 2011-2020. The accumulated points of each team by year is displayed in a column within the dataframe. From this data a three tiered ranking system was developed which split teams into three groupings: 1 (top third percentile), 2 (middle third percentile), and 3 (bottom third percentile). Each team in each season now has a performance ranking attribute applied which is reflective of its total points obtained in that season.


```{r, import and clean team data}
# IMPORT TEAM DATA
c <- c("2011-2012.csv", "2012-2013.csv", "2013-2014.csv", "2014-2015.csv","2015-2016.csv","2016-2017.csv","2017-2018.csv","2018-2019.csv","2019-2020.csv","2020-2021.csv")
url <- paste("https://github.com/HockeyStatSupplier/StatsSupply/blob/main/Team_Standings%20-%20",c,sep="")
team_standings <- lapply(url, function(i){
  l <- read_html(i)
  html_table(l)
})

#iterate through 'team_standings' to create column names from row one of df
allstand <- lapply(team_standings, function(i){  
  df <- as.data.frame(i) #convert list item to dataframe
  df %>% row_to_names(row_number = 1)  
})

#create a list of team points per season
d <- 1:10
pts <- lapply(d, function(i){
  x <- as.data.frame(allstand[i]) #convert list item to dataframe
  as.numeric(x$PTS)
})

#Function to create a numeric three tiered ranking in the points distribution of teams by year
rank_fun = function(p, n = 3){ 
  qtile = quantile(p, probs = seq(0, 1, 1/n))
  sapply(p, function(x) sum(x >= qtile[-(n+1)]))
}

#create a new column in 'allstand' df which indicates rank
sn2 <- lapply(d, function(i){
  x <- as.data.frame(allstand[i]) #convert list item to dataframe
  #apply rank function to create a three tiered list of standings from 1-3
  r <- rank_fun(unlist(pts[i])) 
  #alter Team column nomenclature and add new 'rank' column to dataframe
  x <- x %>% select("PTS") %>% mutate(Team = x$TEAM,PTS = as.numeric(PTS), rank = r)
  orderBy(~Team, x) #order dataframe alphabetically by team in 'Team'
})
```

```{r, display top five rows of cleaned team data}

kable(head(as.data.frame(sn2[1])))
```

### Combine the Player and Team Data

The final dataframe utilized in the models was created by combining the Player and Team dataframes. Several steps were needed to fit the two groupings together: city names in the team data needed to be converted to abbreviations to match the player data. The number of teams and team city names aren't exactly the same for all seasons so this needed to be addressed in the coding.
The final step was to bind the two dataframes, convert variables to numeric where necessary and edit out any unwanted columns that would not be used as features in the model.

```{r, add rankings to Team data and then fit Player and Team Data}

# Create 3 lists containing lists of seasons which apply to 1) Phoenix being a team (2011-2013) 2) Arizona replacing Phoenix (2014-2016) 3) Vegas entering the league (2017 debut)
Phoenix1 <- list(as.data.frame(sn2[1]),as.data.frame(sn2[2]),as.data.frame(sn2[3]))
Arizona1 <- list(sn2[4],sn2[5],sn2[6])
Vegas1 <- list(sn2[7],sn2[8],sn2[9],sn2[10])

# Create sorted list with proper team abbreviations for the seasons with Phoenix
TeamAbb <- sort(unique(as.data.frame(master[5])$TEAM)) #all teams sorted and stored in variable 'TeamAbb'
#Sorted city names in the 3 lists preceding don't pair naturally with their abbreviation counterpart due to order of letters being different for names and abbreviations
#therefore
#index 'TeamAbb' to match the sorted full city names of 'Phoenix1' dataframe 
TeamAbb1 <- TeamAbb[c(1,2,3,6,4,7,8,5,9,10,11,12,13,14,15,17,16,18,19,20,21,22,23,24,25,26,27,28,30,29)]  
p1 <- 1:3
early1 <- lapply(p1, function(i){
  x <- as.data.frame(Phoenix1[i]) #convert list item to dataframe
  x %>% mutate(TEAM = TeamAbb1) #convert city names to abbreviations in 'TEAM'
})

#index 'TeamAbb' to match the sorted full city names of 'Arizona1' dataframe
TeamAbb2 <- TeamAbb[c(1,2,3,4,7,5,8,9,6,10,11,12,13,14,15,16,18,17,19,20,21,22,23,24,25,26,27,28,30,29)]
middle1 <- lapply(p1, function(i){
  x <- as.data.frame(Arizona1[i]) #convert list item to dataframe
  x %>% mutate(TEAM = TeamAbb2) #convert city names to abbreviations in 'TEAM'
})

#Create sorted list with proper team abbreviations for the years with Vegas in the league
TeamAbb3 <- sort(unique(as.data.frame(master[10])$TEAM)) # obtain new abbreviation with Vegas
#index 'TeamAbb3' to match the sorted full city names of 'Vegas1' dataframe
TeamAbb3 <- TeamAbb3[c(1,2,3,4,7,5,8,9,6,10,11,12,13,14,15,16,18,17,19,20,21,22,23,24,25,26,27,28,29,31,30)]
p2 <- 1:4
late1 <- lapply(p2, function(i){
  x <- as.data.frame(Vegas1[i]) #convert list item to dataframe
  x %>% mutate(TEAM = TeamAbb3) #convert city names to abbreviations in 'TEAM'
})

#List of list of teams by year
eml1 <- list(early1[1],early1[2],early1[3],middle1[1],middle1[2],middle1[3],late1[1],late1[2],late1[3],late1[4])
#convert number ranking in 'rank' to character based ranking for clearer data analysis
endlist1 <- lapply(p, function(i){
  x <- as.data.frame(eml1[i]) #convert list item to dataframe
  #split dataframe 'x' into three separate vectors indicating rank
  elite <- as.vector((x %>% filter(rank == 3) %>% select(TEAM))[,1])
  bubble <- as.vector((x %>% filter(rank == 2) %>% select(TEAM))[,1])
  weak <- as.vector((x %>% filter(rank == 1) %>% select(TEAM))[,1])
  y <- as.data.frame(master[i]) #convert list item to dataframe
  #split dataframe 'y' into three separate dataframes with new 'rank' column
  a <- y %>% filter(TEAM %in% elite) %>% mutate(rank = "elite")
  b <- y %>% filter(TEAM %in% bubble) %>% mutate(rank = "bubble")
  c <- y %>% filter(TEAM %in% weak) %>% mutate(rank = "weak")
  d <- rbind(a,b,c) # bind the ranked dataframes
  d[!duplicated(d$Player), ] # remove duplicate rows
})

finaldf <- do.call(rbind,endlist1[1:9]) # convert endlist lists 1:9 into future training dataframe. 
p_2020 <- as.data.frame(endlist1[10])  # convert endlist list 10 containing 2020 season into future test dataframe

#convert feature columns to numeric
cols.num <- c("AGE","CAP.HIT","X..P","GP","G","A","P","PPP")  
finaldf[cols.num] <- sapply(finaldf[cols.num],as.numeric)
p_2020[cols.num] <- sapply(p_2020[cols.num],as.numeric)

kable(head(finaldf))

```

Each observation in the dataset consists of one player's yearly (seasonal) results in 11 categories. The variables that were selected to remain in the final dataframe were:

* Response variable:
    + Rank: team rank of player (Elite, Bubble, Weak)
* Features:
    + AGE: age of player
    + CAP.HIT: adjusted salary of player
    + X..P: points divided by adjusted salary of player
    + GP: Games played in season by player
    + Pos: Position of Player (D,L,R,C)
    + P: number of points scored by player
    + TOI.GP: Average playing (ice) time per game by player
    + PPG: Power Play Goals scored in total by player
    + PPPG: Power Play Points per game by player
    + PPP: Power Play Points scored in total by player
    


### Create training and test sets

The player and team data obtained for this project covers the years 2011 until 2020. For modeling purposes the decision was made to designate the data from years 2011-2019 as the training set and the data from year 2020 as the test set. The training set consists of `r nrow(finaldf)` player values and the test set consists of `r nrow(p_2020)` player values.

```{r, train and test sets}
# create training set and test sets by creating point per game and power play point per game columns and editing out unwanted columns
trains2 <- finaldf %>% mutate(PPG = P/GP, PPPG = PPP/GP) %>% select(- Player,-TEAM,-G,-A,-PPP, -Season) 
tests2 <- p_2020 %>% mutate(PPG = P/GP, PPPG = PPP/GP) %>% select(- Player,-TEAM,-G,-A, -PPP,-Season) 
```


## Exploratory Data Analysis

Exploratory Data Analysis is the initial process of analyzing a dataset to try and gain insight and obtain a summary of the relevant characteristics. In this project the usage of graphs and summary statistics were conducted in order to gain maximum insight into both the player and team data.  

### Distribution of Rank within Dataset

The data is separated into three rankings (Elite, Bubble, and Weak) which are based off of a 10 year sample of all players in the league and the team results for these seasons.


```{r, bargraph}

tab <- trains2 %>%   
  count(rank) %>%  # count the total number of each rank
  mutate(proportion = n/sum(n)) # column to show proportions of rank in the data

#ggplot utilized to create bargraph
tab %>% ggplot(aes(rank, proportion)) + geom_bar(stat = "identity", color = "hot pink1", fill = "dodgerblue1") +  
  labs(
    title = "Distribution of Rank within dataset", fill = element_blank())

```

The bargraph above shows that the highest proportion of the training data is elite and the lowest proportion is weak. We have divided our team standings equally by percentile to create a non biased representation of team so why are we seeing a higher proportion of players that play for elite rather than weak teams? The answer would be found in the selection criteria for the dataset, in particular the minimum games played selector in the player data. As our cut off level for games played is 20 this would indicate that weak teams have more players with under 20 games played per season than elite or bubble teams. From this interpretation we can deduce that elite teams keep a roster of players that remains fairly constant through an entire ~80 game season. Weak teams, on the other hand, tend to have a roster that isn't firmly set and could feature young prospects and fill in players where the expectation level and number of games played is lower on average.


### Distribution of Player Age by Team Rank

In order to search for noticeable difference between ranks we will select the player age feature and visualize the three separate distributions.

```{r, density plot}
#ggplot to show age distribution in density plot, split by facet_wrap into three separate graphs based on rank.
ggplot(trains2, aes(AGE)) + 
  geom_density(colour = "hot pink1", fill = "dodgerblue1") + facet_wrap(~rank) + labs(
    title = "Distribution of Age by rank", fill = element_blank())

```

We see in the density plots expressed above that there is visible difference between the team rankings in accordance to player age distribution.
The age distribution of the weak teams shows a steep peak that is centered at approximately 24 years of age. The bubble teams age distribution peaks at 25 years of age. The elite teams have bimodal age distribution peaks of 24.5 and 27 years of age. Doing some math it appears that the these bimodal values are tied to Position. The highest portion of players for elite forward positions (C,LW,RW) fall within a one year difference around the first peak of 24.5 while the highest proportion of players for elite defence position (D) fall within a one year difference of the second peak 27 years. This phenomenon can be seen in the following density plot example.
\newpage

### Elite Distribution of Age by Forwards and Defencemen
```{r, graph elite forwards and defence age distribution}
#filter data to keep only elite team players.
#ifelse statement to split positions into either Defence or Forward
#ggplot with facet_wrap to create two density plots of age of elite team players representative of position.
trains2 %>% filter(rank == "elite") %>% mutate(Position = ifelse( Pos != "D", "Forward","Defence")) %>% ggplot(aes(AGE)) + geom_density(colour = "grey67", fill = "royalblue2") + facet_wrap(~Position) + labs(
    title = "Elite Distribution of Age by Forwards and Defencemen", fill = element_blank())

```

The visualization above confirms that there is a distinct bimodal distribution of age for the elite teams. The distinction between age and position is quite clear for an elite team. How might this look for the weakly ranked teams? 
\newpage

### Weak Distribution of Age by Forwards and Defencemen

```{r, graph weak forwards and defence age distribution}
#filter data to keep only weak team players.
#ifelse statement to split positions into either Defence or Forward
#ggplot with facet_wrap to create two density plots of age of weak team players representative of position.
trains2 %>% filter(rank == "weak") %>% mutate(Position = ifelse( Pos != "D", "Forward","Defence")) %>% ggplot(aes(AGE)) + geom_density(colour = "blue1", fill = "firebrick2") + facet_wrap(~Position) + labs(
    title = "Weak Distribution of Age by Forwards and Defencemen", fill = element_blank())

```

As we can see in the distribution the age separation for the two positions is not nearly as distinct as with the elite teams. From this information we can infer a correlation between elite teams and age distinction of players by position. An elite team will generally have its highest distribution of forwards around 24.5 years of age and its highest distribution of defencemen around 27 years of age;
\newpage

### Distribution of Points per Game determined by Rank

As the objective of hockey is to score more points than the other team in order to win the match, a player with a high point per game total will be valuable to team success. By using a jitter plot we can see the differences in data between the three rankings of teams.

```{r, jitter plot of ranked ppg distribution}
#Use ggplot to produce jitter plot of ppg by rank with colour representing rank
trains2 %>% ggplot(aes(rank,PPG, colour= rank)) + geom_jitter(width = 0.1, alpha = 0.2) + 
  labs(
    title = "Distribution of PPG by Rank", fill = element_blank())

```

From the visual we can see that the elite column has a much higher overall total of ppg values than the bubble and weak teams. Summarizing the mean ppg per ranking shows elite with the highest average and weak with the lowest average.

```{r, display ppg average by rank}
#group data by rank to utilize summarize function for creation of a ppg average for by rank
x <- trains2 %>% group_by(rank) %>% summarize(avg = mean(PPG))
kable(x[order(desc(x$avg)),]) #display data ordered descending 
```

\newpage

### Point distribution by Age

We have seen how elite teams have the highest mean age and seem the most specific in the relevance of age distribution. How might the PPG be influenced by Age for the three rankings? For this analysis the focus was only on the forward positions (C,R,L) as forwards traditionally are utilized to score points while the defencemen (D) traditionally used to defend against scoring. 

```{r, PPG distribution by Age}

#create a dataframe for each rank that summarize the average ppg by age of forwards.
a <- trains2 %>% filter(Pos != "D" & rank == "elite") %>% group_by(AGE) %>% summarize(ppg = mean(PPG)) %>% mutate(rank = "elite")
b <- trains2 %>% filter(Pos != "D" & rank == "bubble") %>% group_by(AGE) %>% summarize(ppg = mean(PPG)) %>% mutate(rank = "bubble")
c <- trains2 %>% filter(Pos != "D" & rank == "weak") %>% group_by(AGE) %>% summarize(ppg = mean(PPG)) %>% mutate(rank = "weak")
#bind the three dataframes
df <- rbind(a,b,c)
#use ggplot with geom_smooth to create a regression line with confidence bands
df %>% ggplot(aes(AGE,ppg, color = rank)) +  
  geom_smooth(method='lm', formula= y~x) + theme_light() + labs(
    title = "Point per game distribution by Age", x = "Age", y = "Ppg")

```

The above graph for each rank of team displays ppg by age regression lines with confidence bands.
As we can see from the linear interpretation of the data, weak teams are much more reliant upon older players to carry their scoring whereas the elite and bubble teams have a much more even distribution of scoring by age.
\newpage
 
## Model and Tuning Exploration

In this project several models were utilized in an attempt to obtain the best accuracy. These models consisted of Classification and Regression Tree (rpart), Robust Linear Discriminate Analysis (Linda), and eXtreme Gradient Boosting (xgbTree). As there is considerable variability in the time it takes to run these different models, time is included in the results as a consideration of efficiency. The models were performed on both the training set and test set in order to show the similarities and differences in the accuracy obtained for each dataset.

### Step One: Classifier for Baseline Accuracy

Guessing the accuracy can provide a baseline where results are obtained by simple random selection. 

```{r, obtain guess sample from the mean}
# guess rank
set.seed(3, sample.kind = "Rounding")
guess <- sample(c("elite","good","bubble", "weak"), nrow(trains2), replace = TRUE)
train_guess <- mean(guess == trains2$rank)
set.seed(3, sample.kind = "Rounding")
guess <- sample(c("elite","good","bubble", "weak"), nrow(tests2), replace = TRUE)
test_guess <- mean(guess == tests2$rank)
perf_grid = data.frame(Predictor = "Guess Model",
                         "Accuracy (train)" = train_guess,
                         "Accuracy (test)" = test_guess)
kable(perf_grid)
```

Using a system of guessing we obtained an accuracy of `r round(train_guess,3)` for the training data and `r round(test_guess, 3)` for the test data. 


```{r, create function to predict and display results}
accuracy <- function(model, model_name, train, test, tm) {
  train_x <- train %>% select(- rank)
  train_y <- train %>% select(rank)  
  test_x <- test %>% select(- rank)
  test_y <- test %>% select(rank)  
  
  pred_train <- predict(model, train_x)
  pred_test <- predict(model, test_x)
  x <- mean(pred_train == trains2$rank)
  y <- mean(pred_test == tests2$rank)
  
  perf_grid = data.frame(Predictor = c(model_name),
                         "Accuracy (train)" = x,
                         "Accuracy (test)" = y,
                         "Time(secs)" = round(tm, 2))
  perf_grid
}

```


### Step Two: Regression Tree

Regression Tree is the result of a tunable learning algorithm known as rpart. Using the statistical method of recursive partitioning, the rpart algorithm splits data into simpler versions of itself which then continue to split until the expectation of the algorithm parameters are met. The end result of the algorithm is a regression tree which can aid in the classification of data.
Rpart contains one tunable hyperparameter, the complexity parameter, which is set at a default value of 0.01. A tunegrid dataframe containing a sequence of values between 0 and 0.05 was utilized to replace the default value.

```{r, Regression Tree}
ptm <- proc.time()
train_rpart2 <- train(rank ~ ., 
                      method = "rpart",
                      tuneGrid = data.frame(cp = seq(0, 0.05, 0.002)),
                      data = trains2)
rpart.plot(train_rpart2$finalModel, extra = 103)
ptm <- proc.time()
besttune <- train_rpart2$bestTune
grid <- accuracy(train_rpart2, 'Rpart Model', trains2, tests2, ptm[[3]])
grid %>% 
  select(Predictor,"Accuracy (train)" = Accuracy..train., "Accuracy (test)" = Accuracy..test.,"Time (secs)" = Time.secs.) %>%
  kable(booktabs = T) 

```

Each node shows the predicted rank, the predicted probability of a player being of that rank, and the percentage of observations in the node. 
The best tune of the complexity parameter was `r train_rpart2$bestTune`
there is a slight overfit as the train set performs better than the test set.
For the training and test sets we obtained an accuracy of close to 40% which is much higher than the guessing model which is only around 25%. The accuracy obtained isn't particulary high but the nodes of the classification tree can provide insight on the data by determining which features were highlighted in the recursive modeling process. We see PPG, P, PPPG, GP, AGE, and X..P included as the relevant arguments in the decision flow of the classification tree. PPG appears to be important to all rankings while X..P is more relevant in defining the differences between the elite and weak ranking.

### Step Two: Linda

While the rpart method contained one tunable hyperparameter (cp) there are many algorithms which perform without a tuning option, some examples include: Bayesian Generalized Linear Model, Gaussian Process, and Linear Discriminant Analysis. The benefit of these models with no tunable hyperparameters is that results are generally obtained quickly and simply. After testing several models with non-tunable parameters the highest accuracy was found using Robust Linear Discriminant Analysis (Linda).

```{r, Linda}
ptm <- proc.time() 
train_Linda2 <- train(rank ~ ., method = "Linda", data = trains2)
tm <- proc.time() - ptm  
grid2 <- accuracy(train_Linda2, 'Linda model', trains2, tests2, ptm[[3]])
grid2 %>% 
  select(Predictor,"Accuracy (train)" = Accuracy..train., "Accuracy (test)" = Accuracy..test.,"Time (secs)" = Time.secs.) %>%
  kable() 
```

In considering why Linda outperformed the other non tunable algorithms, the Linda model being robust indicates that it is resistant to outliers and thus minimizes error to create a higher accuracy. 

```{r, boxplot of ppg distribution by rank}
trains2 %>%
  ggplot(aes(y = PPG, x = rank)) +
  geom_boxplot(fill = "dodgerblue1", varwidth = T) + 
  labs(
    title = "Boxplot of PPG distribution by team rank",
    x = "Rank", y = "PPG", fill = element_blank()
  ) +
  theme_classic()
```

As shown in the above boxplot there are noticable outliers in the feature of ppg. Outliers are also consistently evident in the featured variables of PPPG, X..P, CAP.HIT and PPP. 


### Step Four: xgbTree

As Linda does not implement tunable hyperparameters the next step was to find a model that is similarly robust but also tunable. An algorithm with tunable hyperparamters can produce model specific optimization when training the data.
XGBoost and boosting in general are very sensitive to outliers, this is because boosting builds each tree on previous trees' residuals/errors.
Outliers will have much larger residuals than non-outliers, so boosting will focus a disproportionate amount of its attention on those points. In order to maximize the available training data and avoid overfitting, all xgbTree models were fit with a train control consisting of cross validation with 3 folds.

#### xgbTree with default parameters

```{r, xgbTree with default parameters}
fit_Control <- caret::trainControl(
  method = "cv", # cross-validation
  number = 3 # with n folds
)
ptm <- proc.time()
xgbTree_default <- train(rank ~ .,
                          data = trains2,
                          method = "xgbTree",
                          trControl = fit_Control)
tm <- proc.time() - ptm
grid3 <-  accuracy(xgbTree_default, 'xgbTree - Default', trains2, tests2, tm[[3]])

grid3 %>% 
  select(Predictor,"Accuracy (train)" = Accuracy..train., "Accuracy (test)" = Accuracy..test.,"Time (secs)" = Time.secs.) %>%
  kable(booktabs = T) 

```

The xgbTree model using the default parameters resulted in a significant increase in accuracy in both training and test sets over the Linda and rpart models. The default tuning parameters for the algorithm are: nrounds = 150, max_depth = 1, eta = 0.3, gamma = 0, colsample_bytree = 0.8, min_child_weight = 1 and subsample = 0.75.

#### xgbTree with tuned hyperparameters

By tuning the hyperparameters, the optimal levels which produce the highest accuracy can be isolated. For this process each parameter was tuned individually with the best result then carried over to the next version of the model. The first step in tuning was to alter the default number of rounds (150) to instead cover a sequence of 100 to 1000 rounds increased by intervals of 50. The learning rate was also expanded to include 0.1, 0.2, 0.3, and 0.4

```{r, tune learning rate xgbTree}
tune_grid <- expand.grid(
  nrounds = seq(from = 100, to = 1000, by = 50),
  eta = c(0.1, 0.2, 0.3, 0.4),  
  max_depth = 1,
  gamma = 0,
  colsample_bytree = 0.8,
  min_child_weight = 1,
  subsample = 0.75
)


ptm <- proc.time()
xgbTree_step1 <- train(rank ~ .,
                        data = trains2,
                        method = "xgbTree",
                        trControl = fit_Control,
                        tuneGrid = tune_grid)
tm <- proc.time() - ptm
grid4 <- accuracy(xgbTree_step1, 'xgbTree - Tune1', trains2, tests2, tm[[3]])

grid4 %>% 
  select(Predictor,"Accuracy (train)" = Accuracy..train., "Accuracy (test)" = Accuracy..test.,"Time (secs)" = Time.secs.) %>%
  kable(booktabs = T) 


```

We see a silght change in the accuracy of the model once the initial hyperparameter tuning was performed. There is minimal increase in accuracy for both the training set and a slight decrease for the test set. The best tune for learning rate was `r xgbTree_step1$bestTune$eta`.

### Maximum Tree Depth

To search for a better fit the maximum tree depth count was then tuned to include all whole numbers between 1 and 5.

```{r, tune max tree depth xgbTree }
tune_grid <- expand.grid(
  nrounds = seq(from = 100, to = 1000, by = 50),
  eta = xgbTree_step1$bestTune$eta,  
  max_depth = c(1, 2, 3, 4, 5),
  gamma = 0,
  colsample_bytree = 0.8,
  min_child_weight = 1,
  subsample = 0.75
)


ptm <- proc.time()
xgbTree_step2 <- train(rank ~ .,
                        data = trains2,
                        method = "xgbTree",
                        trControl = fit_Control,
                        tuneGrid = tune_grid)
tm <- proc.time() - ptm
grid5 <- accuracy(xgbTree_step2, 'xgbTree - Tune2', trains2, tests2, tm[[3]])

grid5 %>% 
  select(Predictor,"Accuracy (train)" = Accuracy..train., "Accuracy (test)" = Accuracy..test.,"Time (secs)" = Time.secs.) %>%
  kable(booktabs = T) 


```

In this model training accuracy has increased and the test set accuracy is slightly decreased. The best tune for maximum tree depth was `r xgbTree_step2$bestTune$max_depth`. 

#### Tuning minimum child weight

For the next step in tuning, the minimum child depth was expanded to include a range of values centered around the default value. In addition, the max depth was once again expanded upon by creating a whole number range of +/- 1 around the best tuning found in the previous model.

```{r, minimum child weight tuned xgbTree}
tune_grid <- expand.grid(
  nrounds = seq(from = 100, to = 1000, by = 50),
  eta = xgbTree_step1$bestTune$eta,
  max_depth = c(xgbTree_step2$bestTune$max_depth - 1,   xgbTree_step2$bestTune$max_depth, xgbTree_step2$bestTune$max_depth + 1), 
  gamma = 0,
  colsample_bytree = 0.8,
  min_child_weight = c(0.1, 0.25, 0.5, 1),
  subsample = 0.75
)

ptm <- proc.time()
xgbTree_step3 <- train(rank ~ .,
                        data = trains2,
                        method = "xgbTree",
                        trControl = fit_Control,
                        tuneGrid = tune_grid)
tm <- proc.time() - ptm
grid6 <- accuracy(xgbTree_step3, 'xgbTree - Tune3', trains2, tests2, tm[[3]])

grid6 %>% 
  select(Predictor,"Accuracy (train)" = Accuracy..train., "Accuracy (test)" = Accuracy..test.,"Time (secs)" = Time.secs.) %>%
  kable(booktabs = T) 

```

With this tuning variation there was a substantial increase in the training accuracy while the test accuracy decreased slightly. The best tune for minimum child weight in this model was `r xgbTree_step3$bestTune$min_child_weight` and the best tune for max depth was `r xgbTree_step3$bestTune$max_depth`.

#### Tuning subsample ratio of columns and observations

In the next step, the best tune values of learning rate, max tree depth and minimum child weight were added to a new tuning grid which also contained multiple values for subsample ratio of columns and training observations. This process creates different random samples of the training data prior to growing trees which can help prevent overfitting.

```{r, subsample tuned xgbTree}
tune_grid <- expand.grid(
  nrounds = seq(from = 100, to = 1000, by = 50),
  eta = xgbTree_step1$bestTune$eta,
  max_depth = xgbTree_step3$bestTune$max_depth,
  gamma = 0,
  colsample_bytree = c(0.6, 0.8, 1.0),
  min_child_weight = xgbTree_step3$bestTune$min_child_weight,
  subsample = seq(.5, 1, length = .1)
)


ptm <- proc.time()
xgbTree_step4 <- train(rank ~ .,
                        data = trains2,
                        method = "xgbTree",
                        trControl = fit_Control,
                        tuneGrid = tune_grid)
tm <- proc.time() - ptm
grid7 <- accuracy(xgbTree_step4, 'xgbTree - Tune4', trains2, tests2, tm[[3]])

grid7 %>% 
  select(Predictor,"Accuracy (train)" = Accuracy..train., "Accuracy (test)" = Accuracy..test.,"Time (secs)" = Time.secs.) %>%
  kable(booktabs = T) 


```

After tuning these hyperparameters there was a decrease in training accuracy but the test accuracy was slightly elevated. The results are consistent in the expectation that training accuracy would decrease from reduced overfitting but in turn the test accuracy should be boosted.  The best tune for column subsample in this model was `r xgbTree_step4$bestTune$colsample_bytree` and the best tune for subsample ratio of training observations was `r xgbTree_step4$bestTune$subsample`


#### Tuning the Gamma hyperparameter

Gamma is a regularization parameter for xgbTree which represents how much the loss will be reduced by for the model to perform a split resulting in a tree.The higher the level of Gamma the higher the degree of regularization in the model. In this step the model was performed with a degree of Gamma sequenced from 0 to 1 by an interval of 0.5. 

```{r, gamma tuned xgbTree}
tune_grid <- expand.grid(
  nrounds = seq(from = 100, to = 1000, by = 50),
  eta = xgbTree_step1$bestTune$eta,
  max_depth = xgbTree_step3$bestTune$max_depth,
  gamma = seq(from = 0, to = 1, by = 0.5),
  colsample_bytree = xgbTree_step4$bestTune$colsample_bytree,
  min_child_weight = xgbTree_step3$bestTune$min_child_weight,
  subsample = xgbTree_step4$bestTune$subsample
)

ptm <- proc.time()
xgbTree_step5 <- train(rank ~ .,
                        data = trains2,
                        method = "xgbTree",
                        trControl = fit_Control,
                        tuneGrid = tune_grid)
tm <- proc.time() - ptm
grid8 <- accuracy(xgbTree_step5, 'xgbTree - Tune5', trains2, tests2, tm[[3]])

grid8 %>% 
  select(Predictor,"Accuracy (train)" = Accuracy..train., "Accuracy (test)" = Accuracy..test.,"Time (secs)" = Time.secs.) %>%
  kable(booktabs = T) 
```

By altering the Gamma values the best tuned model had a decrease in the training and test set accuracy.


# Results

The table below shows our results through all explored versions of our model.

```{r, bind all results dataframes}
x <- rbind(grid,grid2,grid3,grid4,grid5,grid6,grid7,grid8)
x  %>% kable(booktabs = T)  
```

The third tuning of xgbTree hyperparameters produced the best results for the training set accuracy and the best test accuracy was achieved in the initial version of xgbTree. The best model has surpassed the training set classifier for baseline accuracy by `r round(grid6$Accuracy..train. - train_guess,3)`

The Variable Importance for the best training model is represented as:

```{r, plot variable importance}
importance <- varImp(xgbTree_step3)
plot(importance)
```

Four top variables really stand out for importance (X..P, TOI.GP, CAP.HIT, PPG), while four variables are middling (PPPG, P, GP, AGE) and two variables are low importance (PosD, PosL). PosR is also listed but the value is essentially negligible.

A team that is seeking to attain elite status should consider focusing resources and attention accordingly on these top features. If you want your team to be elite a positive strategy would be to model ice time of players in the same way as other elite teams, with an extra emphasis on Defence and Left Wing positions. This strategy could entail spending extra money to hire the best coach who is in charge of defencemen. A team could also consider spending above budget to obtain the best free agent Left Wing player who fits closely with the peak age of the elite forwards. You want to have a Cost per Point (X..P) similar to the elites and you can evaluate your players when offering salary contract extensions in that regard. 

As the accuracy certainly isn't perfect we can't say for certain that a team will have more success if they focus on these areas, however we have found a significant correlation between team performance and these features. By studying the features of the elite teams, a performance expectation of the players can be more clearly defined. Further testing could be conducted between variables to gain more insight. For example, if a correlation between age and average ice time for elite teams vs weak teams is identified, a weight potentially could be applied to the model.

*R version 4.1.0 was used for the modeling in this project in RStudio for Windows. Slightly different accuracy was found in xgbTree modeling when using earlier versions of R in Rstudio for MacOS. To be sure results precisely match please use R version 4.1.0.
\newpage

# Conclusion

In this project publicly available nhl player and team data was imported, engineered, analyzed through graphs and summary statistics and then utilized to make predictions through machine learning models. In comparing the models that were used, two separate tuned versions of eXtreme Gradient Boosting provided the best results for training and test accuracy. The most important feature to predict rank in the training data was the cost per point variable.
By examining the results, particularly the variable importance, a National Hockey League team can find insights on how to achieve greater success. 
The accuracy obtained in the project doesn't guarantee success in adaptation of the analysis presented, however further examination of correlation between features may provide even better predictive results.




